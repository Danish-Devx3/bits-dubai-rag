services:
  # ---------------------------------------------------------------------------
  # Vector Database: Qdrant
  # Used for storing vector embeddings for RAG.
  # ---------------------------------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: bits_qdrant
    restart: always
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:6333/healthz" ]
      interval: 5s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # Local Ollama (for embeddings)
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: bits_ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  # Pull embedding model on startup
  ollama-init:
    image: curlimages/curl
    container_name: bits_ollama_init
    depends_on:
      - ollama
    restart: "no"
    command: >
      sh -c "echo 'Waiting for Ollama...' &&
             sleep 10 &&
             echo 'Pulling bge-m3 embedding model...' &&
             curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"bge-m3\"}' &&
             echo 'Embedding model ready!'"

  # ---------------------------------------------------------------------------
  # Backend: NestJS
  # ---------------------------------------------------------------------------
  backend:
    image: ddon8889722/backend-v1:latest
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: bits_backend
    restart: always
    ports:
      - "3001:3001"
    environment:
      # Database
      - DATABASE_URL=mongodb+srv://bits:bits123@youtube.v7xaz1a.mongodb.net/bits_dubai?retryWrites=true&w=majority&appName=Youtube
      - PORT=3001
      - FRONTEND_URL=http://localhost:3000
      - JWT_SECRET=dev-secret

      # Local Ollama for EMBEDDINGS
      - OLLAMA_EMBEDDING_URL=http://ollama:11434
      - OLLAMA_EMBEDDING_MODEL=bge-m3

      # Cloud LLM API
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-https://api.ollama.com}
      - OLLAMA_API_KEY=${OLLAMA_API_KEY}
      - OLLAMA_LLM_MODEL=deepseek-v3.1:671b-cloud

      # Qdrant Vector Database
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION=bits_dubai_knowledge_base
    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_started

  # ---------------------------------------------------------------------------
  # Frontend: Next.js
  # ---------------------------------------------------------------------------
  frontend:
    image: ddon8889722/frontend-v1:latest
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_BACKEND_URL=http://localhost:3001
    container_name: bits_frontend
    restart: always
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL:-http://localhost:3001}
    depends_on:
      - backend

volumes:
  qdrant_data:
  ollama_data:
